"""
Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª | Feature Engineering | IngÃ©nierie des Features
Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆØ­Ø³Ø§Ø¨ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù…Ù† Ø§Ù„Ø£Ø­Ø¯Ø§Ø« Ø§Ù„Ø®Ø§Ù…
Extraction et calcul des features Ã  partir des Ã©vÃ©nements bruts
"""

import os
import json
import math
import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple, Any
from collections import defaultdict, deque
from datetime import datetime
from pathlib import Path
import logging

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ | Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class FeatureExtractor:
    """
    Ù…Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª | Extracteur de Features
    ÙŠØ­Ø³Ø¨ 15+ Ù…ÙŠØ²Ø© Ù…Ù† Ø§Ù„Ø£Ø­Ø¯Ø§Ø« Ø¹Ù„Ù‰ Ù†Ø§ÙØ°Ø© Ù…ØªØ­Ø±ÙƒØ©
    Calcule 15+ features Ã  partir des Ã©vÃ©nements sur une fenÃªtre glissante
    """
    
    def __init__(self, window_size: float = 10.0):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬ | Initialisation de l'extracteur
        
        Args:
            window_size: Ø­Ø¬Ù… Ø§Ù„Ù†Ø§ÙØ°Ø© Ø§Ù„Ù…ØªØ­Ø±ÙƒØ© Ø¨Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ | Taille de la fenÃªtre en secondes
        """
        self.window_size = window_size
        self._event_buffer: deque = deque()
        self._last_features: Optional[Dict] = None
        
        logger.info(f"ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù…Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª | Extracteur initialisÃ©: window={window_size}s")
    
    # ==================== Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ù†ØªØ±ÙˆØ¨ÙŠØ§ ====================
    # ==================== Calcul de l'Entropie ====================
    
    def _calculate_entropy(self, values: List[str]) -> float:
        """
        Ø­Ø³Ø§Ø¨ Ø¥Ù†ØªØ±ÙˆØ¨ÙŠØ§ Ø´Ø§Ù†ÙˆÙ† | Calculer l'entropie de Shannon
        
        Args:
            values: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù‚ÙŠÙ… | Liste des valeurs
            
        Returns:
            Ù‚ÙŠÙ…Ø© Ø§Ù„Ø¥Ù†ØªØ±ÙˆØ¨ÙŠØ§ | Valeur de l'entropie
        """
        if not values:
            return 0.0
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª | Calculer les frÃ©quences
        freq = defaultdict(int)
        for v in values:
            freq[v] += 1
        
        total = len(values)
        entropy = 0.0
        
        for count in freq.values():
            if count > 0:
                p = count / total
                entropy -= p * math.log2(p)
        
        return entropy
    
    # ==================== Ø­Ø³Ø§Ø¨ Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„Ø§Ù†ÙØ¬Ø§Ø± ====================
    # ==================== Calcul du Coefficient de Burstiness ====================
    
    def _calculate_burstiness(self, timestamps: List[float]) -> float:
        """
        Ø­Ø³Ø§Ø¨ Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„Ø§Ù†ÙØ¬Ø§Ø± | Calculer le coefficient de burstiness
        ÙŠÙ‚ÙŠØ³ Ù…Ø¯Ù‰ ØªØ¬Ù…Ø¹ Ø§Ù„Ø£Ø­Ø¯Ø§Ø« ÙÙŠ ÙØªØ±Ø§Øª Ù‚ØµÙŠØ±Ø©
        Mesure le degrÃ© de regroupement des Ã©vÃ©nements
        
        Args:
            timestamps: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø£ÙˆÙ‚Ø§Øª | Liste des timestamps
            
        Returns:
            Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„Ø§Ù†ÙØ¬Ø§Ø± (0-1) | Coefficient de burstiness
        """
        if len(timestamps) < 2:
            return 0.0
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ÙØªØ±Ø§Øª Ø¨ÙŠÙ† Ø§Ù„Ø£Ø­Ø¯Ø§Ø« | Calculer les intervalles
        sorted_ts = sorted(timestamps)
        intervals = [sorted_ts[i+1] - sorted_ts[i] for i in range(len(sorted_ts)-1)]
        
        if not intervals:
            return 0.0
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø· ÙˆØ§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ | Calculer moyenne et Ã©cart-type
        mean = np.mean(intervals)
        std = np.std(intervals)
        
        if mean == 0:
            return 0.0
        
        # Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„Ø§Ù†ÙØ¬Ø§Ø± = (std - mean) / (std + mean)
        # Ù‚ÙŠÙ… Ù…ÙˆØ¬Ø¨Ø© ØªØ¹Ù†ÙŠ Ø§Ù†ÙØ¬Ø§Ø±ØŒ Ø³Ø§Ù„Ø¨Ø© ØªØ¹Ù†ÙŠ Ø§Ù†ØªØ¸Ø§Ù…
        burstiness = (std - mean) / (std + mean) if (std + mean) > 0 else 0
        
        # ØªØ·Ø¨ÙŠØ¹ Ø¥Ù„Ù‰ 0-1 | Normaliser Ã  0-1
        return (burstiness + 1) / 2
    
    # ==================== Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª ====================
    # ==================== Extraction des Features ====================
    
    def extract_features_from_events(self, events: List[Dict]) -> Dict[str, float]:
        """
        Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù…Ù† Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø£Ø­Ø¯Ø§Ø«
        Extraire toutes les features d'une liste d'Ã©vÃ©nements
        
        Args:
            events: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø£Ø­Ø¯Ø§Ø« | Liste des Ã©vÃ©nements
            
        Returns:
            Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„Ù…ÙŠØ²Ø§Øª | Dictionnaire des features
        """
        if not events:
            return self._get_empty_features()
        
        # ØªØµÙ†ÙŠÙ Ø§Ù„Ø£Ø­Ø¯Ø§Ø« Ø­Ø³Ø¨ Ø§Ù„Ù…ØµØ¯Ø± | Classifier par source
        process_events = [e for e in events if e.get('source') == 'process']
        network_events = [e for e in events if e.get('source') == 'network']
        file_events = [e for e in events if e.get('source') == 'file']
        
        # Ø§Ù„Ø£ÙˆÙ‚Ø§Øª | Timestamps
        all_timestamps = [e.get('timestamp', 0) for e in events]
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø¯Ø© | Calculer la durÃ©e
        if all_timestamps:
            duration = (max(all_timestamps) - min(all_timestamps)) / 1000  # Ø¨Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ
            duration = max(duration, 0.001)  # ØªØ¬Ù†Ø¨ Ø§Ù„Ù‚Ø³Ù…Ø© Ø¹Ù„Ù‰ ØµÙØ±
        else:
            duration = self.window_size
        
        features = {}
        
        # ==================== Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ù„ÙØ§Øª ====================
        # ==================== Features Fichiers ====================
        
        # 1. Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ø§Ù„Ø«Ø§Ù†ÙŠØ© | OpÃ©rations fichiers par seconde
        features['file_ops_per_sec'] = len(file_events) / duration
        
        # 2. Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙØ±ÙŠØ¯Ø© | Ratio fichiers uniques
        file_paths = [e.get('data', {}).get('path', '') for e in file_events]
        unique_files = len(set(file_paths))
        features['unique_files_ratio'] = unique_files / max(len(file_paths), 1)
        
        # 3. Ù†Ø³Ø¨Ø© Ø§Ù„Ø­Ø°Ù/Ø§Ù„Ø¥Ù†Ø´Ø§Ø¡ | Ratio suppression/crÃ©ation
        create_ops = sum(1 for e in file_events if e.get('data', {}).get('operation') == 'created')
        delete_ops = sum(1 for e in file_events if e.get('data', {}).get('operation') == 'deleted')
        features['delete_create_ratio'] = delete_ops / max(create_ops, 1)
        
        # 4. Ø¥Ù†ØªØ±ÙˆØ¨ÙŠØ§ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª | Entropie des chemins
        features['path_entropy'] = self._calculate_entropy(file_paths)
        
        # 5. Ø¥Ù†ØªØ±ÙˆØ¨ÙŠØ§ Ø§Ù„Ø§Ù…ØªØ¯Ø§Ø¯Ø§Øª | Entropie des extensions
        extensions = [e.get('data', {}).get('extension', '') for e in file_events]
        features['file_extension_entropy'] = self._calculate_entropy(extensions)
        
        # ==================== Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª ====================
        # ==================== Features Processus ====================
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª | Extraire les donnÃ©es processus
        cpu_values = []
        memory_values = []
        io_read_values = []
        io_write_values = []
        
        for e in process_events:
            data = e.get('data', {})
            if 'cpu_percent' in data:
                cpu_values.append(data['cpu_percent'])
            if 'memory_percent' in data:
                memory_values.append(data['memory_percent'])
            if 'io_read_bytes' in data:
                io_read_values.append(data['io_read_bytes'])
            if 'io_write_bytes' in data:
                io_write_values.append(data['io_write_bytes'])
        
        # 6. Ù…ØªÙˆØ³Ø· CPU | Moyenne CPU
        features['cpu_mean'] = np.mean(cpu_values) if cpu_values else 0.0
        
        # 7. Ø§Ù†Ø­Ø±Ø§Ù CPU | Ã‰cart-type CPU
        features['cpu_std'] = np.std(cpu_values) if cpu_values else 0.0
        
        # 8. Ù…ØªÙˆØ³Ø· Ø§Ù„Ø°Ø§ÙƒØ±Ø© | Moyenne mÃ©moire
        features['memory_mean'] = np.mean(memory_values) if memory_values else 0.0
        
        # 9. Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© | Taux de lecture
        if io_read_values and len(io_read_values) > 1:
            io_read_rate = (max(io_read_values) - min(io_read_values)) / duration
        else:
            io_read_rate = 0.0
        features['io_read_rate'] = io_read_rate / 1_000_000  # MB/s
        
        # 10. Ù…Ø¹Ø¯Ù„ Ø§Ù„ÙƒØªØ§Ø¨Ø© | Taux d'Ã©criture
        if io_write_values and len(io_write_values) > 1:
            io_write_rate = (max(io_write_values) - min(io_write_values)) / duration
        else:
            io_write_rate = 0.0
        features['io_write_rate'] = io_write_rate / 1_000_000  # MB/s
        
        # 11. Ø¹Ø¯Ù… ØªÙ…Ø§Ø«Ù„ I/O | AsymÃ©trie I/O
        total_io = features['io_read_rate'] + features['io_write_rate']
        if total_io > 0:
            features['io_asymmetry'] = abs(features['io_read_rate'] - features['io_write_rate']) / total_io
        else:
            features['io_asymmetry'] = 0.0
        
        # ==================== Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø´Ø¨ÙƒØ© ====================
        # ==================== Features RÃ©seau ====================
        
        # 12. Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª | Taux de connexions
        features['net_connections_rate'] = len(network_events) / duration
        
        # 13. Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ù†Ø§ÙØ° Ø§Ù„ÙØ±ÙŠØ¯Ø© | Ratio ports uniques
        remote_ports = []
        for e in network_events:
            data = e.get('data', {})
            if 'unique_remote_ports' in data:
                remote_ports.append(data['unique_remote_ports'])
        features['unique_ports_ratio'] = np.mean(remote_ports) if remote_ports else 0.0
        
        # 14. Ø§Ù†ÙØ¬Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª | Burst de connexions
        net_timestamps = [e.get('timestamp', 0) for e in network_events]
        features['connection_burst'] = self._calculate_burstiness(net_timestamps)
        
        # ==================== Ù…ÙŠØ²Ø§Øª Ø²Ù…Ù†ÙŠØ© ====================
        # ==================== Features Temporelles ====================
        
        # 15. Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„Ø§Ù†ÙØ¬Ø§Ø± Ø§Ù„Ø¹Ø§Ù… | Coefficient de burstiness global
        features['burstiness'] = self._calculate_burstiness(all_timestamps)
        
        # 16. Ø§Ù„Ø§Ù†ØªØ¸Ø§Ù… Ø§Ù„Ø²Ù…Ù†ÙŠ | RÃ©gularitÃ© temporelle
        if len(all_timestamps) > 2:
            sorted_ts = sorted(all_timestamps)
            intervals = [sorted_ts[i+1] - sorted_ts[i] for i in range(len(sorted_ts)-1)]
            if intervals:
                cv = np.std(intervals) / np.mean(intervals) if np.mean(intervals) > 0 else 0
                features['temporal_regularity'] = 1 / (1 + cv)  # Ø£Ø¹Ù„Ù‰ = Ø£ÙƒØ«Ø± Ø§Ù†ØªØ¸Ø§Ù…Ø§Ù‹
            else:
                features['temporal_regularity'] = 0.5
        else:
            features['temporal_regularity'] = 0.5
        
        # ==================== Ù…ÙŠØ²Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© ====================
        # ==================== Features SupplÃ©mentaires ====================
        
        # 17. ÙƒØ«Ø§ÙØ© Ø§Ù„Ø£Ø­Ø¯Ø§Ø« | DensitÃ© d'Ã©vÃ©nements
        features['event_density'] = len(events) / duration
        
        # 18. ØªÙ†ÙˆØ¹ Ø§Ù„Ù…ØµØ§Ø¯Ø± | DiversitÃ© des sources
        sources = [e.get('source', '') for e in events]
        features['source_diversity'] = len(set(sources)) / 3  # max 3 sources
        
        self._last_features = features
        return features
    
    def _get_empty_features(self) -> Dict[str, float]:
        """
        Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…ÙŠØ²Ø§Øª ÙØ§Ø±ØºØ© | Obtenir des features vides
        """
        return {
            'file_ops_per_sec': 0.0,
            'unique_files_ratio': 0.0,
            'delete_create_ratio': 0.0,
            'path_entropy': 0.0,
            'file_extension_entropy': 0.0,
            'cpu_mean': 0.0,
            'cpu_std': 0.0,
            'memory_mean': 0.0,
            'io_read_rate': 0.0,
            'io_write_rate': 0.0,
            'io_asymmetry': 0.0,
            'net_connections_rate': 0.0,
            'unique_ports_ratio': 0.0,
            'connection_burst': 0.0,
            'burstiness': 0.0,
            'temporal_regularity': 0.5,
            'event_density': 0.0,
            'source_diversity': 0.0
        }
    
    def get_feature_names(self) -> List[str]:
        """
        Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª | Obtenir les noms des features
        """
        return list(self._get_empty_features().keys())
    
    def add_event(self, event: Dict):
        """
        Ø¥Ø¶Ø§ÙØ© Ø­Ø¯Ø« Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø®Ø²Ù† Ø§Ù„Ù…Ø¤Ù‚Øª | Ajouter un Ã©vÃ©nement au buffer
        """
        self._event_buffer.append(event)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø­Ø¯Ø§Ø« Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© | Supprimer les anciens Ã©vÃ©nements
        current_time = event.get('timestamp', 0)
        cutoff = current_time - (self.window_size * 1000)  # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ù…ÙŠÙ„ÙŠ Ø«Ø§Ù†ÙŠØ©
        
        while self._event_buffer and self._event_buffer[0].get('timestamp', 0) < cutoff:
            self._event_buffer.popleft()
    
    def get_current_features(self) -> Dict[str, float]:
        """
        Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ© Ù…Ù† Ø§Ù„Ù…Ø®Ø²Ù† Ø§Ù„Ù…Ø¤Ù‚Øª
        Obtenir les features actuelles du buffer
        """
        return self.extract_features_from_events(list(self._event_buffer))
    
    def clear_buffer(self):
        """Ù…Ø³Ø­ Ø§Ù„Ù…Ø®Ø²Ù† Ø§Ù„Ù…Ø¤Ù‚Øª | Vider le buffer"""
        self._event_buffer.clear()


class DatasetFeatureProcessor:
    """
    Ù…Ø¹Ø§Ù„Ø¬ Ù…ÙŠØ²Ø§Øª Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª | Processeur de Features du Dataset
    ÙŠØ­ÙˆÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù… Ø¥Ù„Ù‰ Ù…ÙŠØ²Ø§Øª Ø¬Ø§Ù‡Ø²Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨
    Transforme le dataset brut en features prÃªtes pour l'entraÃ®nement
    """
    
    def __init__(self, window_size: float = 10.0, step_size: float = 1.0):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ | Initialisation du processeur
        
        Args:
            window_size: Ø­Ø¬Ù… Ø§Ù„Ù†Ø§ÙØ°Ø© Ø¨Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ | Taille de la fenÃªtre en secondes
            step_size: Ø­Ø¬Ù… Ø§Ù„Ø®Ø·ÙˆØ© Ø¨Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ | Taille du pas en secondes
        """
        self.window_size = window_size
        self.step_size = step_size
        self.extractor = FeatureExtractor(window_size=window_size)
        
        logger.info(f"ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª | Processeur initialisÃ©: window={window_size}s, step={step_size}s")
    
    def process_jsonl_file(
        self,
        input_file: str,
        output_file: Optional[str] = None
    ) -> pd.DataFrame:
        """
        Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù JSONL ÙˆØªØ­ÙˆÙŠÙ„Ù‡ Ø¥Ù„Ù‰ DataFrame Ù…Ø¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª
        Traiter un fichier JSONL et le convertir en DataFrame avec features
        
        Args:
            input_file: Ù…Ù„Ù Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ | Fichier d'entrÃ©e
            output_file: Ù…Ù„Ù Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ) | Fichier de sortie (optionnel)
            
        Returns:
            DataFrame Ù…Ø¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª | DataFrame avec features
        """
        logger.info(f"Ù…Ø¹Ø§Ù„Ø¬Ø© | Traitement: {input_file}")
        
        # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø£Ø­Ø¯Ø§Ø« | Lire les Ã©vÃ©nements
        events = []
        with open(input_file, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    event = json.loads(line.strip())
                    events.append(event)
                except json.JSONDecodeError:
                    continue
        
        logger.info(f"ØªÙ… Ù‚Ø±Ø§Ø¡Ø© {len(events)} Ø­Ø¯Ø« | {len(events)} Ã©vÃ©nements lus")
        
        if not events:
            return pd.DataFrame()
        
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„ÙˆÙ‚Øª | Trier par temps
        events.sort(key=lambda x: x.get('timestamp', 0))
        
        # ØªÙ‚Ø³ÙŠÙ… Ø¥Ù„Ù‰ Ù†ÙˆØ§ÙØ° | Diviser en fenÃªtres
        features_list = []
        labels = []
        
        min_ts = events[0].get('timestamp', 0)
        max_ts = events[-1].get('timestamp', 0)
        
        window_ms = self.window_size * 1000
        step_ms = self.step_size * 1000
        
        current_start = min_ts
        
        while current_start + window_ms <= max_ts:
            # Ø¬Ù…Ø¹ Ø£Ø­Ø¯Ø§Ø« Ø§Ù„Ù†Ø§ÙØ°Ø© | Collecter les Ã©vÃ©nements de la fenÃªtre
            window_events = [
                e for e in events
                if current_start <= e.get('timestamp', 0) < current_start + window_ms
            ]
            
            if window_events:
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª | Extraire les features
                features = self.extractor.extract_features_from_events(window_events)
                features['window_start'] = current_start
                features['window_end'] = current_start + window_ms
                features['event_count'] = len(window_events)
                features_list.append(features)
                
                # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ØªØ³Ù…ÙŠØ© (Ø§Ù„Ø£ØºÙ„Ø¨ÙŠØ©) | DÃ©terminer le label (majoritÃ©)
                window_labels = [e.get('label', 'benign') for e in window_events]
                malicious_count = sum(1 for l in window_labels if l == 'malicious')
                label = 'malicious' if malicious_count > len(window_labels) / 2 else 'benign'
                labels.append(label)
            
            current_start += step_ms
        
        # Ø¥Ù†Ø´Ø§Ø¡ DataFrame | CrÃ©er le DataFrame
        df = pd.DataFrame(features_list)
        df['label'] = labels
        df['label_numeric'] = df['label'].map({'benign': 0, 'malicious': 1})
        
        logger.info(f"ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ {len(df)} Ù†Ø§ÙØ°Ø© | {len(df)} fenÃªtres crÃ©Ã©es")
        
        # Ø­ÙØ¸ Ø¥Ø°Ø§ Ø·ÙÙ„Ø¨ | Sauvegarder si demandÃ©
        if output_file:
            df.to_csv(output_file, index=False)
            logger.info(f"ØªÙ… Ø§Ù„Ø­ÙØ¸ ÙÙŠ | SauvegardÃ©: {output_file}")
        
        return df
    
    def get_feature_statistics(self, df: pd.DataFrame) -> Dict:
        """
        Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…ÙŠØ²Ø§Øª | Obtenir les statistiques des features
        """
        feature_cols = self.extractor.get_feature_names()
        
        stats = {
            'total_samples': len(df),
            'benign_samples': len(df[df['label'] == 'benign']),
            'malicious_samples': len(df[df['label'] == 'malicious']),
            'features': {}
        }
        
        for col in feature_cols:
            if col in df.columns:
                stats['features'][col] = {
                    'mean': float(df[col].mean()),
                    'std': float(df[col].std()),
                    'min': float(df[col].min()),
                    'max': float(df[col].max())
                }
        
        return stats
    
    def print_statistics(self, df: pd.DataFrame):
        """
        Ø·Ø¨Ø§Ø¹Ø© Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…ÙŠØ²Ø§Øª | Afficher les statistiques
        """
        stats = self.get_feature_statistics(df)
        
        print("\n" + "=" * 70)
        print("ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…ÙŠØ²Ø§Øª | Statistiques des Features")
        print("=" * 70)
        print(f"ğŸ“¦ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª | Total Ã©chantillons: {stats['total_samples']}")
        print(f"ğŸŒ¿ Ø­Ù…ÙŠØ¯Ø© | BÃ©nins: {stats['benign_samples']}")
        print(f"ğŸ”´ Ù…Ø´Ø¨ÙˆÙ‡Ø© | Malveillants: {stats['malicious_samples']}")
        print("\nğŸ“ˆ Ø§Ù„Ù…ÙŠØ²Ø§Øª | Features:")
        print("-" * 70)
        print(f"{'Feature':<30} {'Mean':>10} {'Std':>10} {'Min':>10} {'Max':>10}")
        print("-" * 70)
        
        for name, values in stats['features'].items():
            print(f"{name:<30} {values['mean']:>10.3f} {values['std']:>10.3f} "
                  f"{values['min']:>10.3f} {values['max']:>10.3f}")
        
        print("=" * 70)


# Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ÙˆØ­Ø¯Ø© | Test du module
if __name__ == "__main__":
    print("=" * 60)
    print("Ø§Ø®ØªØ¨Ø§Ø± Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª | Test Feature Engineering")
    print("=" * 60)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø£Ø­Ø¯Ø§Ø« ÙˆÙ‡Ù…ÙŠØ© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø± | CrÃ©er des Ã©vÃ©nements factices
    import time
    
    extractor = FeatureExtractor(window_size=10)
    
    # Ù…Ø­Ø§ÙƒØ§Ø© Ø£Ø­Ø¯Ø§Ø« | Simuler des Ã©vÃ©nements
    test_events = []
    base_time = time.time() * 1000
    
    # Ø£Ø­Ø¯Ø§Ø« Ù…Ù„ÙØ§Øª | Ã‰vÃ©nements fichiers
    for i in range(50):
        test_events.append({
            'timestamp': base_time + i * 100,
            'source': 'file',
            'data': {
                'operation': 'created' if i % 3 != 0 else 'deleted',
                'path': f'/test/file_{i % 10}.txt',
                'extension': '.txt'
            },
            'label': 'benign'
        })
    
    # Ø£Ø­Ø¯Ø§Ø« Ø¹Ù…Ù„ÙŠØ§Øª | Ã‰vÃ©nements processus
    for i in range(30):
        test_events.append({
            'timestamp': base_time + i * 150,
            'source': 'process',
            'data': {
                'cpu_percent': 10 + i % 20,
                'memory_percent': 30 + i % 10,
                'io_read_bytes': 1000000 + i * 10000,
                'io_write_bytes': 500000 + i * 5000
            },
            'label': 'benign'
        })
    
    # Ø£Ø­Ø¯Ø§Ø« Ø´Ø¨ÙƒØ© | Ã‰vÃ©nements rÃ©seau
    for i in range(20):
        test_events.append({
            'timestamp': base_time + i * 200,
            'source': 'network',
            'data': {
                'unique_remote_ports': i % 10
            },
            'label': 'benign'
        })
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª | Extraire les features
    features = extractor.extract_features_from_events(test_events)
    
    print("\nğŸ“Š Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø© | Features Extraites:")
    print("-" * 40)
    for name, value in features.items():
        print(f"  {name}: {value:.4f}")
    
    print(f"\nâœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(features)} Ù…ÙŠØ²Ø© | {len(features)} features extraites")
