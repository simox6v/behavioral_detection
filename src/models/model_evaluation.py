"""
ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ | Ã‰valuation des ModÃ¨les | Model Evaluation
ØªÙ‚ÙŠÙŠÙ… Ø´Ø§Ù…Ù„ Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…Ø¹ Ø±Ø³ÙˆÙ… Ø¨ÙŠØ§Ù†ÙŠØ©
Ã‰valuation complÃ¨te des performances avec visualisations
"""

import os
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Optional, Tuple, Any
from pathlib import Path
from datetime import datetime
import logging
import joblib

from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_curve, auc,
    precision_recall_curve, average_precision_score
)

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ | Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø±Ø³ÙˆÙ… | Configuration des graphiques
plt.style.use('seaborn-v0_8-darkgrid')
plt.rcParams['figure.figsize'] = (10, 6)
plt.rcParams['font.size'] = 12


class ModelEvaluator:
    """
    Ù…Ù‚ÙŠÙ‘Ù… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ | Ã‰valuateur de ModÃ¨les
    ÙŠÙ‚ÙŠÙ… Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆÙŠÙˆÙ„Ø¯ ØªÙ‚Ø§Ø±ÙŠØ± Ø´Ø§Ù…Ù„Ø©
    Ã‰value les performances et gÃ©nÃ¨re des rapports complets
    """
    
    def __init__(self, output_dir: Optional[str] = None):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ù‚ÙŠÙ‘Ù… | Initialisation de l'Ã©valuateur
        
        Args:
            output_dir: Ù…Ø¬Ù„Ø¯ Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± | RÃ©pertoire des rapports
        """
        self.output_dir = Path(output_dir or './data/evaluation')
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.evaluation_results: Dict[str, Dict] = {}
        
        logger.info(f"ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ù‚ÙŠÙ‘Ù… | Ã‰valuateur initialisÃ©: {self.output_dir}")
    
    def evaluate_model(
        self,
        model_name: str,
        model: Any,
        X_test: np.ndarray,
        y_test: np.ndarray,
        is_anomaly_detector: bool = False
    ) -> Dict:
        """
        ØªÙ‚ÙŠÙŠÙ… Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ø­Ø¯ | Ã‰valuer un modÃ¨le
        
        Args:
            model_name: Ø§Ø³Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ | Nom du modÃ¨le
            model: Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ | Le modÃ¨le
            X_test: Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± | DonnÃ©es de test
            y_test: Ø§Ù„ØªØ³Ù…ÙŠØ§Øª Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© | Labels rÃ©els
            is_anomaly_detector: Ù‡Ù„ Ù‡Ùˆ ÙƒØ§Ø´Ù Ø´Ø°ÙˆØ° | Est un dÃ©tecteur d'anomalies
            
        Returns:
            Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… | RÃ©sultats d'Ã©valuation
        """
        logger.info(f"ğŸ“Š ØªÙ‚ÙŠÙŠÙ… | Ã‰valuation: {model_name}")
        
        # Ø§Ù„ØªÙ†Ø¨Ø¤ | PrÃ©diction
        if is_anomaly_detector:
            y_pred_raw = model.predict(X_test)
            # ØªØ­ÙˆÙŠÙ„: -1 -> 1 (malicious), 1 -> 0 (benign)
            y_pred = np.where(y_pred_raw == -1, 1, 0)
            
            # Ø§Ù„Ø¯Ø±Ø¬Ø§Øª Ø¥Ø°Ø§ ØªÙˆÙØ±Øª | Scores si disponibles
            if hasattr(model, 'score_samples'):
                y_scores = -model.score_samples(X_test)  # Ø¹ÙƒØ³ Ù„Ù„Ø´Ø°ÙˆØ°
            elif hasattr(model, 'decision_function'):
                y_scores = -model.decision_function(X_test)
            else:
                y_scores = None
        else:
            y_pred = model.predict(X_test)
            if hasattr(model, 'predict_proba'):
                y_scores = model.predict_proba(X_test)[:, 1]
            else:
                y_scores = None
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ | Calculer les mÃ©triques
        results = {
            'model_name': model_name,
            'accuracy': float(accuracy_score(y_test, y_pred)),
            'precision': float(precision_score(y_test, y_pred, zero_division=0)),
            'recall': float(recall_score(y_test, y_pred, zero_division=0)),
            'f1': float(f1_score(y_test, y_pred, zero_division=0)),
            'confusion_matrix': confusion_matrix(y_test, y_pred).tolist(),
            'classification_report': classification_report(y_test, y_pred, output_dict=True)
        }
        
        # Ø­Ø³Ø§Ø¨ Ù…Ø¹Ø¯Ù„Ø§Øª Ø§Ù„Ø®Ø·Ø£ | Calculer les taux d'erreur
        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
        results['true_positives'] = int(tp)
        results['true_negatives'] = int(tn)
        results['false_positives'] = int(fp)
        results['false_negatives'] = int(fn)
        results['false_positive_rate'] = float(fp / (fp + tn)) if (fp + tn) > 0 else 0
        results['false_negative_rate'] = float(fn / (fn + tp)) if (fn + tp) > 0 else 0
        results['specificity'] = float(tn / (tn + fp)) if (tn + fp) > 0 else 0
        
        # AUC Ùˆ ROC | AUC et ROC
        if y_scores is not None:
            try:
                fpr, tpr, thresholds = roc_curve(y_test, y_scores)
                results['auc'] = float(auc(fpr, tpr))
                results['roc_curve'] = {
                    'fpr': fpr.tolist(),
                    'tpr': tpr.tolist(),
                    'thresholds': thresholds.tolist()
                }
                
                # Precision-Recall curve
                precision, recall, pr_thresholds = precision_recall_curve(y_test, y_scores)
                results['average_precision'] = float(average_precision_score(y_test, y_scores))
                results['pr_curve'] = {
                    'precision': precision.tolist(),
                    'recall': recall.tolist()
                }
            except Exception as e:
                logger.warning(f"Ø®Ø·Ø£ ÙÙŠ Ø­Ø³Ø§Ø¨ AUC | Erreur AUC: {e}")
                results['auc'] = 0.0
        
        self.evaluation_results[model_name] = results
        
        return results
    
    def evaluate_all_models(
        self,
        models: Dict[str, Any],
        X_test: np.ndarray,
        y_test: np.ndarray,
        anomaly_models: List[str] = None
    ) -> Dict[str, Dict]:
        """
        ØªÙ‚ÙŠÙŠÙ… Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ | Ã‰valuer tous les modÃ¨les
        """
        if anomaly_models is None:
            anomaly_models = ['isolation_forest', 'one_class_svm', 'lof']
        
        for name, model in models.items():
            if model is not None:
                is_anomaly = name in anomaly_models
                self.evaluate_model(name, model, X_test, y_test, is_anomaly_detector=is_anomaly)
        
        return self.evaluation_results
    
    # ==================== Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ© ====================
    # ==================== Visualisations ====================
    
    def plot_confusion_matrix(
        self,
        model_name: str,
        save: bool = True
    ) -> Optional[plt.Figure]:
        """
        Ø±Ø³Ù… Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ù„ØªØ¨Ø§Ø³ | Tracer la matrice de confusion
        """
        if model_name not in self.evaluation_results:
            logger.warning(f"Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ | ModÃ¨le non trouvÃ©: {model_name}")
            return None
        
        cm = np.array(self.evaluation_results[model_name]['confusion_matrix'])
        
        fig, ax = plt.subplots(figsize=(8, 6))
        sns.heatmap(
            cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Benign (0)', 'Malicious (1)'],
            yticklabels=['Benign (0)', 'Malicious (1)'],
            ax=ax
        )
        ax.set_xlabel('Predicted | Ø§Ù„ØªÙ†Ø¨Ø¤', fontsize=12)
        ax.set_ylabel('Actual | Ø§Ù„ÙØ¹Ù„ÙŠ', fontsize=12)
        ax.set_title(f'Confusion Matrix - {model_name}\nÙ…ØµÙÙˆÙØ© Ø§Ù„Ø§Ù„ØªØ¨Ø§Ø³', fontsize=14)
        
        plt.tight_layout()
        
        if save:
            filepath = self.output_dir / f'confusion_matrix_{model_name}.png'
            plt.savefig(filepath, dpi=150, bbox_inches='tight')
            logger.info(f"ØªÙ… Ø­ÙØ¸ | SauvegardÃ©: {filepath}")
        
        return fig
    
    def plot_roc_curves(self, save: bool = True) -> Optional[plt.Figure]:
        """
        Ø±Ø³Ù… Ù…Ù†Ø­Ù†ÙŠØ§Øª ROC Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ | Tracer les courbes ROC
        """
        fig, ax = plt.subplots(figsize=(10, 8))
        
        colors = plt.cm.Set2(np.linspace(0, 1, len(self.evaluation_results)))
        
        for (name, results), color in zip(self.evaluation_results.items(), colors):
            if 'roc_curve' in results:
                fpr = results['roc_curve']['fpr']
                tpr = results['roc_curve']['tpr']
                auc_score = results.get('auc', 0)
                ax.plot(fpr, tpr, color=color, lw=2,
                       label=f'{name} (AUC = {auc_score:.3f})')
        
        ax.plot([0, 1], [0, 1], 'k--', lw=2, label='Random')
        ax.set_xlim([0, 1])
        ax.set_ylim([0, 1.05])
        ax.set_xlabel('False Positive Rate | Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ§Øª Ø§Ù„ÙƒØ§Ø°Ø¨Ø©', fontsize=12)
        ax.set_ylabel('True Positive Rate | Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ§Øª Ø§Ù„ØµØ­ÙŠØ­Ø©', fontsize=12)
        ax.set_title('ROC Curves - Ù…Ù†Ø­Ù†ÙŠØ§Øª ROC', fontsize=14)
        ax.legend(loc='lower right')
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save:
            filepath = self.output_dir / 'roc_curves.png'
            plt.savefig(filepath, dpi=150, bbox_inches='tight')
            logger.info(f"ØªÙ… Ø­ÙØ¸ | SauvegardÃ©: {filepath}")
        
        return fig
    
    def plot_metrics_comparison(self, save: bool = True) -> Optional[plt.Figure]:
        """
        Ø±Ø³Ù… Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ | Tracer la comparaison des mÃ©triques
        """
        metrics = ['accuracy', 'precision', 'recall', 'f1']
        models = list(self.evaluation_results.keys())
        
        data = []
        for model in models:
            for metric in metrics:
                value = self.evaluation_results[model].get(metric, 0)
                data.append({
                    'Model': model,
                    'Metric': metric.capitalize(),
                    'Value': value
                })
        
        df = pd.DataFrame(data)
        
        fig, ax = plt.subplots(figsize=(12, 6))
        
        x = np.arange(len(models))
        width = 0.2
        
        for i, metric in enumerate(metrics):
            values = df[df['Metric'] == metric.capitalize()]['Value'].values
            ax.bar(x + i * width, values, width, label=metric.capitalize())
        
        ax.set_xlabel('Model | Ø§Ù„Ù†Ù…ÙˆØ°Ø¬', fontsize=12)
        ax.set_ylabel('Score | Ø§Ù„Ø¯Ø±Ø¬Ø©', fontsize=12)
        ax.set_title('Metrics Comparison | Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³', fontsize=14)
        ax.set_xticks(x + width * 1.5)
        ax.set_xticklabels(models, rotation=45, ha='right')
        ax.legend()
        ax.set_ylim([0, 1.1])
        ax.grid(True, alpha=0.3, axis='y')
        
        plt.tight_layout()
        
        if save:
            filepath = self.output_dir / 'metrics_comparison.png'
            plt.savefig(filepath, dpi=150, bbox_inches='tight')
            logger.info(f"ØªÙ… Ø­ÙØ¸ | SauvegardÃ©: {filepath}")
        
        return fig
    
    def plot_false_rates(self, save: bool = True) -> Optional[plt.Figure]:
        """
        Ø±Ø³Ù… Ù…Ø¹Ø¯Ù„Ø§Øª Ø§Ù„Ø®Ø·Ø£ | Tracer les taux d'erreur
        """
        models = list(self.evaluation_results.keys())
        fpr = [self.evaluation_results[m].get('false_positive_rate', 0) for m in models]
        fnr = [self.evaluation_results[m].get('false_negative_rate', 0) for m in models]
        
        fig, ax = plt.subplots(figsize=(10, 6))
        
        x = np.arange(len(models))
        width = 0.35
        
        bars1 = ax.bar(x - width/2, fpr, width, label='False Positive Rate | Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ§Øª Ø§Ù„ÙƒØ§Ø°Ø¨Ø©', color='coral')
        bars2 = ax.bar(x + width/2, fnr, width, label='False Negative Rate | Ø§Ù„Ø³Ù„Ø¨ÙŠØ§Øª Ø§Ù„ÙƒØ§Ø°Ø¨Ø©', color='steelblue')
        
        ax.set_xlabel('Model | Ø§Ù„Ù†Ù…ÙˆØ°Ø¬', fontsize=12)
        ax.set_ylabel('Rate | Ø§Ù„Ù…Ø¹Ø¯Ù„', fontsize=12)
        ax.set_title('Error Rates Comparison | Ù…Ù‚Ø§Ø±Ù†Ø© Ù…Ø¹Ø¯Ù„Ø§Øª Ø§Ù„Ø®Ø·Ø£', fontsize=14)
        ax.set_xticks(x)
        ax.set_xticklabels(models, rotation=45, ha='right')
        ax.legend()
        ax.set_ylim([0, 1])
        ax.grid(True, alpha=0.3, axis='y')
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù‚ÙŠÙ… ÙÙˆÙ‚ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© | Ajouter les valeurs sur les barres
        for bar, val in zip(bars1, fpr):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
                   f'{val:.2f}', ha='center', va='bottom', fontsize=9)
        for bar, val in zip(bars2, fnr):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
                   f'{val:.2f}', ha='center', va='bottom', fontsize=9)
        
        plt.tight_layout()
        
        if save:
            filepath = self.output_dir / 'error_rates.png'
            plt.savefig(filepath, dpi=150, bbox_inches='tight')
            logger.info(f"ØªÙ… Ø­ÙØ¸ | SauvegardÃ©: {filepath}")
        
        return fig
    
    def generate_report(self, save: bool = True) -> str:
        """
        ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± Ù†ØµÙŠ Ø´Ø§Ù…Ù„ | GÃ©nÃ©rer un rapport textuel complet
        """
        report_lines = []
        report_lines.append("=" * 70)
        report_lines.append("ğŸ“Š ØªÙ‚Ø±ÙŠØ± ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ | Rapport d'Ã‰valuation des ModÃ¨les")
        report_lines.append("=" * 70)
        report_lines.append(f"Ø§Ù„ØªØ§Ø±ÙŠØ® | Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append("")
        
        for model_name, results in self.evaluation_results.items():
            report_lines.append("-" * 70)
            report_lines.append(f"ğŸ”¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ | ModÃ¨le: {model_name.upper()}")
            report_lines.append("-" * 70)
            report_lines.append(f"   Accuracy (Ø§Ù„Ø¯Ù‚Ø©): {results['accuracy']:.4f}")
            report_lines.append(f"   Precision (Ø§Ù„Ø¥Ø­ÙƒØ§Ù…): {results['precision']:.4f}")
            report_lines.append(f"   Recall (Ø§Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡): {results['recall']:.4f}")
            report_lines.append(f"   F1 Score: {results['f1']:.4f}")
            if 'auc' in results:
                report_lines.append(f"   AUC: {results['auc']:.4f}")
            report_lines.append(f"   False Positive Rate: {results['false_positive_rate']:.4f}")
            report_lines.append(f"   False Negative Rate: {results['false_negative_rate']:.4f}")
            report_lines.append("")
            report_lines.append("   Confusion Matrix | Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ù„ØªØ¨Ø§Ø³:")
            cm = np.array(results['confusion_matrix'])
            report_lines.append(f"   TN={cm[0,0]:5d}  FP={cm[0,1]:5d}")
            report_lines.append(f"   FN={cm[1,0]:5d}  TP={cm[1,1]:5d}")
            report_lines.append("")
        
        # Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬ | Meilleur modÃ¨le
        report_lines.append("=" * 70)
        report_lines.append("ğŸ† Ø§Ù„Ø®Ù„Ø§ØµØ© | Conclusion")
        report_lines.append("=" * 70)
        
        best_f1 = max(self.evaluation_results.items(), key=lambda x: x[1]['f1'])
        best_accuracy = max(self.evaluation_results.items(), key=lambda x: x[1]['accuracy'])
        lowest_fpr = min(self.evaluation_results.items(), key=lambda x: x[1]['false_positive_rate'])
        
        report_lines.append(f"   Ø£ÙØ¶Ù„ F1 | Meilleur F1: {best_f1[0]} ({best_f1[1]['f1']:.4f})")
        report_lines.append(f"   Ø£ÙØ¶Ù„ Accuracy | Meilleure Accuracy: {best_accuracy[0]} ({best_accuracy[1]['accuracy']:.4f})")
        report_lines.append(f"   Ø£Ù‚Ù„ FPR | Plus bas FPR: {lowest_fpr[0]} ({lowest_fpr[1]['false_positive_rate']:.4f})")
        report_lines.append("=" * 70)
        
        report = "\n".join(report_lines)
        
        if save:
            filepath = self.output_dir / 'evaluation_report.txt'
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(report)
            logger.info(f"ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± | Rapport sauvegardÃ©: {filepath}")
        
        return report
    
    def save_results(self, filename: str = "evaluation_results.json"):
        """
        Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙƒÙ€ JSON | Sauvegarder les rÃ©sultats en JSON
        """
        filepath = self.output_dir / filename
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªÙˆØ§ÙÙ‚ Ù…Ø¹ JSON
        serializable_results = {}
        for model_name, results in self.evaluation_results.items():
            serializable_results[model_name] = {}
            for key, value in results.items():
                if isinstance(value, np.ndarray):
                    serializable_results[model_name][key] = value.tolist()
                elif isinstance(value, (np.float32, np.float64)):
                    serializable_results[model_name][key] = float(value)
                elif isinstance(value, (np.int32, np.int64)):
                    serializable_results[model_name][key] = int(value)
                else:
                    serializable_results[model_name][key] = value
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ | RÃ©sultats sauvegardÃ©s: {filepath}")
    
    def generate_all_visualizations(self):
        """
        ØªÙˆÙ„ÙŠØ¯ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ© | GÃ©nÃ©rer toutes les visualisations
        """
        logger.info("ğŸ“ˆ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ© | GÃ©nÃ©ration des visualisations...")
        
        # Ù…ØµÙÙˆÙØ§Øª Ø§Ù„Ø§Ù„ØªØ¨Ø§Ø³ | Matrices de confusion
        for model_name in self.evaluation_results.keys():
            self.plot_confusion_matrix(model_name)
        
        # Ù…Ù†Ø­Ù†ÙŠØ§Øª ROC | Courbes ROC
        self.plot_roc_curves()
        
        # Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ | Comparaison des mÃ©triques
        self.plot_metrics_comparison()
        
        # Ù…Ø¹Ø¯Ù„Ø§Øª Ø§Ù„Ø®Ø·Ø£ | Taux d'erreur
        self.plot_false_rates()
        
        plt.close('all')
        
        logger.info("âœ… ØªÙ… ØªÙˆÙ„ÙŠØ¯ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø±Ø³ÙˆÙ… | Toutes les visualisations gÃ©nÃ©rÃ©es")


def main():
    """
    Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© | Fonction principale
    """
    import argparse
    
    parser = argparse.ArgumentParser(
        description="ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ | Ã‰valuation des ModÃ¨les"
    )
    parser.add_argument(
        '--models-dir',
        type=str,
        default='./data/models',
        help='Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ | RÃ©pertoire des modÃ¨les'
    )
    parser.add_argument(
        '--data',
        type=str,
        required=True,
        help='Ù…Ù„Ù Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± | Fichier de donnÃ©es de test'
    )
    parser.add_argument(
        '--output',
        type=str,
        default='./data/evaluation',
        help='Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ | RÃ©pertoire de sortie'
    )
    
    args = parser.parse_args()
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª | Charger les donnÃ©es
    df = pd.read_csv(args.data)
    exclude_cols = ['label', 'label_numeric', 'window_start', 'window_end', 'event_count']
    feature_cols = [c for c in df.columns if c not in exclude_cols]
    
    X_test = df[feature_cols].values
    y_test = df['label_numeric'].values
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ | Charger le scaler
    models_dir = Path(args.models_dir)
    scaler_files = list(models_dir.glob('scaler*.joblib'))
    if scaler_files:
        scaler = joblib.load(scaler_files[-1])
        X_test = scaler.transform(X_test)
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ | Charger les modÃ¨les
    models = {}
    model_types = ['isolation_forest', 'one_class_svm', 'lof', 'random_forest', 'xgboost']
    
    for model_type in model_types:
        model_files = list(models_dir.glob(f'{model_type}*.joblib'))
        if model_files:
            models[model_type] = joblib.load(model_files[-1])
    
    # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ | Ã‰valuer les modÃ¨les
    evaluator = ModelEvaluator(output_dir=args.output)
    evaluator.evaluate_all_models(models, X_test, y_test)
    
    # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± | GÃ©nÃ©rer les rapports
    report = evaluator.generate_report()
    print(report)
    
    evaluator.generate_all_visualizations()
    evaluator.save_results()
    
    logger.info("âœ… ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ | TerminÃ©")


if __name__ == "__main__":
    main()
